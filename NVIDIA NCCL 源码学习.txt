https://blog.csdn.net/kidgin7439/category_11998768.html  和知乎同名的

NVIDIA NCCL 源码学习（十）- 多机间ncclSend和ncclRecv的过程
https://blog.csdn.net/KIDGIN7439/article/details/130936177
回忆一下单机的执行流程，用户执行ncclSend之后通过ncclEnqueueCheck将sendbuff，sendbytes，peer等信息保存到了comm->p2plist中；
然后执行ncclGroupEnd，如果发现channel没有建立到peer的链接则先建链，
然后根据p2plist执行scheduleSendRecv(ncclSaveKernel)将信息保存到channel->collectives，
然后再启动kernel，kernel会遍历channel->collectives执行send和recv。然后我们看下多机的流程是怎样的。
最后总结下多机通信的整体流程
通信由kernel和proxy线程协调完成，send端kernel负责将数据从input搬运到buf，proxy线程负责将buf中数据通过网络发送给recv端
kernel和proxy间通过队列实现生产者消费者模式
send端通过rdma send发送数据，和recv端通过队列实现生产者消费者模式，队列位于send端，recv端每次下发一个wr到rq之后会执行rdma write通知send端

NVIDIA NCCL 源码学习（九）- 单机内ncclSend和ncclRecv的过程
https://blog.csdn.net/KIDGIN7439/article/details/128326053
单机内的通信都是通过kernel来进行的，所以整个通信的过程可以分为两步，第一步是准备kernel相关的参数，第二步是实际执行kernel的过程。

NVIDIA NCCL 源码学习（八）- 数据通信链路transport的建立
https://blog.csdn.net/kidgin7439/article/details/126953432
最后简单总结下，建链的过程都是以下过程：
接收端 执行recv setup，创建buffer等，将相关信息记录到connectIndo，启动一个监听socket，ip port同样记录到connectInfo，通过bootstrap发送connectInfo到 发送端。
发送端 执行send setup，创建buffer等，将相关信息记录到connectInfo，然后发送给 接收端。这一步rdma场景没有用到connectInfo。
发送端 接受到步骤1中 接收端 的信息，然后建立 发送端 到 接收端 的链接，p2p场景的话只是简单记录对端buffer，rdma场景的话需要初始化qp到INIT状态。
接收端 接受到步骤2中send发送的信息，然后建立 接收端 到 发送端 的链接，p2p场景还是记录对端buffer，rdma场景需要初始化qp到RTS状态，将本端的qp信息发送回对端。
如果rdma场景的话，发送端 还需接收对端的qp状态初始化本端的qp到RTS状态。

NVIDIA NCCL 源码学习（七）- 机器间channel连接
https://blog.csdn.net/KIDGIN7439/article/details/128144057

NVIDIA NCCL 源码学习（六）- channel搜索
https://zhuanlan.zhihu.com/p/653440728
==上节讲到已经计算出GPU和NIC节点到其他任意节点的最优路径了，本节看下NCCL中channel的搜索过程。
nccl中channel的概念表示一个通信路径，为了更好的利用带宽和网卡，以及同一块数据可以通过多个channel并发通信，
另外后续可以看到一个channel对应了一个GPU SM， 所以基于这些原因，nccl会使用多channel，搜索的过程就是搜索出来一组channel。
如上节所述，单机的情况下会在ncclTopoTrimSystem函数里删除网卡，
因此我们先看下单机八卡这种简化的情况，最后再看下多机引入网卡之后的情况。
总结一下，本节就是基于机器拓扑，搜索出一组channel用于数据的通信，并记录到ncclTopoGraph。

NVIDIA NCCL 源码学习（五）- 路径计算
https://my.oschina.net/oneflow/blog/10089670
https://blog.csdn.net/kidgin7439/article/details/127849771
上节NCCL完成了对机器PCI系统拓扑的建图，其中建好的图如下所示，其中GPU之间是通过NVLink连接起来的
003-001.png
为了方便之后的搜索channel，接下来NCCL会先计算GPU和NIC节点到其他任意节点之间的最优路径，
以及对应的带宽，即最优路径上所有边的带宽的最小值。
那么抽象一下，这个问题可以建模为给定一个无向图，每条边有一个权值，给定查询(u, v)，求节点u到节点v的路径，
使得路径上的最小边的权值最大，类似无向图的最小瓶颈路，可以用生成树+LCA的方法解决；如果查询中的u是固定的，
那么也可以使用类似SPFA的方法解决，将松弛方法改一下即可。


NVIDIA NCCL 源码学习（四）- 建图过程
https://zhuanlan.zhihu.com/p/640812018
==上次分析了NCCL对机器PCI系统进行拓扑分析的过程，产出的结果为xml格式，
接下来，NCCL会根据这个xml进图的建立过程以便之后进行路径搜索。
总结下，由于拓扑分析产出的xml不便于进行后续的路径搜索，所以本节基于xml对PCI系统进行了建图。

NVIDIA NCCL 源码学习（三）- 机器内拓扑分析
https://zhuanlan.zhihu.com/p/625606436
==上节介绍所有节点执行了bootstrap网络连接的建立，接下来介绍下拓扑分析。
==由于GPU机器架构是多种多样的，一台机器上可能有多个网卡，多个GPU卡，卡间连接也各不相同，
==因此需要对机器内设备连接拓扑进行分析，以使性能在各种拓扑结构下都尽可能好

在看具体拓扑分析流程之前先简单了解一下PCIe的一些概念，一个简单的PCIe系统示例如下。
002-001.png
switch的作用是扩展PCIe端口，下边可以连接设备或者其他switch，上游来的请求被被他转发，
PCIe设备可以连在RC，也可以连在swtich，一个switch的内部如下所示。
002-002.png
内部有一个PCIe总线 ，然后通过多个Bridge扩展出多个端口，其中上边的那个称为上游端口，其他的叫做下游端口。
前文有提到NCCL中很常用的一个变量名叫busId，比如gpu和ib网卡，
注意区分NCCL里的busId并不是指的总线号，指的其实是定位一个PCIe设备用到的id，
即BDF(bus + device + function)，一个bus上有多个设备，一个设备有多个功能，
因此通过BDF就可以定位一个设备，
在机器启动完成PCIe的配置之后会将相关信息通过sysfs提供给用户，
NCCL就是通过sysfs来完成拓扑检测的。

总结一下，本节主要介绍了NCCL拓扑分析的过程，通过sysfs将gpu和网卡对应的pci树结构建立出来了xml树。

NVIDIA NCCL 源码学习（二）- bootstrap网络连接的建立
https://zhuanlan.zhihu.com/p/620499558
==所有节点间bootstrap的连接是如何建立的

NVIDIA NCCL 源码学习（一）- 初始化及ncclUniqueId的产生
https://zhuanlan.zhihu.com/p/614746112
==介绍到rank0的机器生成了ncclUniqueId，并完成了机器的bootstrap网络和通信网络的初始化