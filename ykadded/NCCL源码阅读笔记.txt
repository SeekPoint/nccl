
https://lgd.gd/2021/03/21/NCCL%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
NCCL源码阅读笔记
代码框架
nccl-master-src
├── Makefile
├── include
├── collectives 		// 集合通信原语的实现
├── graph						// 检测网络拓扑结构
├── transport				// 与数据传输相关的函数实现
├── bootstrap.cc		// （我的理解是）很多内建的helper function
├── channel.cc
├── debug.cc
├── enqueue.cc			// 关于队列的操作
├── group.cc				// NCCL group API的实现
├── init.cc					// 初始化的代码
├── proxy.cc				// Proxy线程相关的代码
└── transport.cc		// 数据传输相关
目前我读到的一些代码以及相应的功能已经标注在文件树中。
关于底层的数据传输、怎样建立socket通信的代码，主要集中在transport.cc proxy.cc以及transport文件夹中。
关于上层的集合通信原语的实现，例如Allreduce, Allgather等，主要集中在collectives文件夹中。
...
proxy可以看做是在CPU端运行了一个当前GPU跟其他GPU通信的代理，由proxy来决定通过何种方式传输数据。（采用netTransport的）Proxy会运行一个驻留线程。循环检查队列中有无op。op->progress则是每个op对应的proxy操作。
跨CPU的数据通信可以采用Socket、Infiniband等机制。
GPU上的kernel内部也有一个（待传输数据的）队列。这个队列是怎样维护的？
    这个队列不需要维护。在launch kernel之后，kernel会计算要传输的数据的地址和大小，
    修改ncclConnInfo 这个结构体中的数据队列指针*tail，
    然后调用同步函数，等待proxy线程传输刚刚添加到*tail中的数据。
    因此，数据传输过程其实是GPU上的NCCL kernel跟CPU上的Proxy线程协同完成的。
Proxy线程中与数据传输有关的三个函数是
    ncclNetIrecv：Proxy从网络收到数据
    ncclNetIsend：Proxy发送数据到网络
    ncclNetIflush: Proxy将数据从CPU传到GPU
    在NCCL中没有这三个函数的实现，应该是调用了外部API。
数据怎样从一个GPU传输到同一个节点的另一个GPU？
    Peer-to-peer, PCI+host memory
数据怎样从一个GPU传输到另一个节点的GPU？
    Socket, InfiniBand
...
关于NCCL call在GPU上执行的方式：
The NCCL call returns when the operation has been effectively enqueued to the given stream,
or returns an error. The collective operation is then executed asynchronously on the CUDA device.

类似于TensorFlow中，各个operator的执行流程。先是enqueue到一个给定的stream。
然后在GPU上异步地执行，The operation status can be queried using standard CUDA semantics,
for example, calling cudaStreamSynchronize or using CUDA events。

总结
总的来说，NCCL的工作流程如下：
首先，每个要参与数据传输的GPU都要调用ncclCommInitRank创建一个与其rank对应的Communicator，同一个communication group中的每个communicator具有相同的unique ID。
当每个设备调用ncclCommInitRank时，设备之间会交换一些信息，例如各自的IP，bus ID等。然后检测整个系统中的网络拓扑结构。
有了网络拓扑结构，NCCL会进一步搜索当前网络中最佳的RING、TREE、COLLNET图结构。
有了设备之间的图结构信息，就可以在存在通路的设备之间建立点对点的连接。主要有三种连接方式：p2p，shared memory以及network。采用哪种方式取决于这两个节点之间支持怎样的连接方式。
以上就是初始化阶段的所有准备工作。
初始化完成后，就可以调用集合通信原语。例如ncclAllReduce。集合通信函数会被enqueue到一个CUDA stream上，在GPU上异步执行。
接下来在CPU上启动Proxy线程，作为GPU上集合通信kernel的代理，与GPU kernel协同完成与其他设备之间的数据传输。GPU kernel负责计算所需传输的数据的地址以及数据量大小，而Proxy线程负责完成实际的数据传输。对于采用p2pTransport 以及shmTransport的设备，在建立连接后可以直接传输数据，对于采用netTransport的设备，则需要通过socket进行数据传输。
# NCCL

