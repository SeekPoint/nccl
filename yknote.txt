NCCL 资料
NCCL官方说明：
https://devblogs.nvidia.com/fast-multi-gpu-collectives-nccl/
https://developer.nvidia.com/nccl
http://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/index.html

https://blog.csdn.net/litdaguang/article/details/55259389
Fast Multi-GPU collectives with NCCL-翻译


https://zhuanlan.zhihu.com/p/609121884
【微软】MSCCL Github仓库介绍
https://github.com/Mellanox/nccl-rdma-sharp-plugins
nccl-rdma-sharp plugin enables RDMA and Switch based collectives(SHARP) with NVIDIA's NCCL library
https://docs.nvidia.com/networking/display/HPCXv27/NCCL-RDMA-SHARP+Plugins

AMD的对应 https://github.com/ROCmSoftwarePlatform/rccl



============================================================================================================================================================


https://lgd.gd/2021/03/21/NCCL%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
NCCL源码阅读笔记
代码框架
nccl-master-src
├── Makefile
├── include
├── collectives 		// 集合通信原语的实现
├── graph						// 检测网络拓扑结构
├── transport				// 与数据传输相关的函数实现
├── bootstrap.cc		// （我的理解是）很多内建的helper function
├── channel.cc
├── debug.cc
├── enqueue.cc			// 关于队列的操作
├── group.cc				// NCCL group API的实现
├── init.cc					// 初始化的代码
├── proxy.cc				// Proxy线程相关的代码
└── transport.cc		// 数据传输相关
目前我读到的一些代码以及相应的功能已经标注在文件树中。
关于底层的数据传输、怎样建立socket通信的代码，主要集中在transport.cc proxy.cc以及transport文件夹中。
关于上层的集合通信原语的实现，例如Allreduce, Allgather等，主要集中在collectives文件夹中。
...
proxy可以看做是在CPU端运行了一个当前GPU跟其他GPU通信的代理，由proxy来决定通过何种方式传输数据。（采用netTransport的）Proxy会运行一个驻留线程。循环检查队列中有无op。op->progress则是每个op对应的proxy操作。
跨CPU的数据通信可以采用Socket、Infiniband等机制。
GPU上的kernel内部也有一个（待传输数据的）队列。这个队列是怎样维护的？
    这个队列不需要维护。在launch kernel之后，kernel会计算要传输的数据的地址和大小，
    修改ncclConnInfo 这个结构体中的数据队列指针*tail，
    然后调用同步函数，等待proxy线程传输刚刚添加到*tail中的数据。
    因此，数据传输过程其实是GPU上的NCCL kernel跟CPU上的Proxy线程协同完成的。
Proxy线程中与数据传输有关的三个函数是
    ncclNetIrecv：Proxy从网络收到数据
    ncclNetIsend：Proxy发送数据到网络
    ncclNetIflush: Proxy将数据从CPU传到GPU
    在NCCL中没有这三个函数的实现，应该是调用了外部API。
数据怎样从一个GPU传输到同一个节点的另一个GPU？
    Peer-to-peer, PCI+host memory
数据怎样从一个GPU传输到另一个节点的GPU？
    Socket, InfiniBand
...
关于NCCL call在GPU上执行的方式：
The NCCL call returns when the operation has been effectively enqueued to the given stream,
or returns an error. The collective operation is then executed asynchronously on the CUDA device.

类似于TensorFlow中，各个operator的执行流程。先是enqueue到一个给定的stream。
然后在GPU上异步地执行，The operation status can be queried using standard CUDA semantics,
for example, calling cudaStreamSynchronize or using CUDA events。

总结
总的来说，NCCL的工作流程如下：
首先，每个要参与数据传输的GPU都要调用ncclCommInitRank创建一个与其rank对应的Communicator，同一个communication group中的每个communicator具有相同的unique ID。
当每个设备调用ncclCommInitRank时，设备之间会交换一些信息，例如各自的IP，bus ID等。然后检测整个系统中的网络拓扑结构。
有了网络拓扑结构，NCCL会进一步搜索当前网络中最佳的RING、TREE、COLLNET图结构。
有了设备之间的图结构信息，就可以在存在通路的设备之间建立点对点的连接。主要有三种连接方式：p2p，shared memory以及network。采用哪种方式取决于这两个节点之间支持怎样的连接方式。
以上就是初始化阶段的所有准备工作。
初始化完成后，就可以调用集合通信原语。例如ncclAllReduce。集合通信函数会被enqueue到一个CUDA stream上，在GPU上异步执行。
接下来在CPU上启动Proxy线程，作为GPU上集合通信kernel的代理，与GPU kernel协同完成与其他设备之间的数据传输。GPU kernel负责计算所需传输的数据的地址以及数据量大小，而Proxy线程负责完成实际的数据传输。对于采用p2pTransport 以及shmTransport的设备，在建立连接后可以直接传输数据，对于采用netTransport的设备，则需要通过socket进行数据传输。
# NCCL

=======================================================================================================================================================
https://blog.csdn.net/kidgin7439/category_11998768.html  和知乎同名的
NVIDIA NCCL 源码学习（十）- 多机间ncclSend和ncclRecv的过程
https://blog.csdn.net/KIDGIN7439/article/details/130936177
回忆一下单机的执行流程，用户执行ncclSend之后通过ncclEnqueueCheck将sendbuff，sendbytes，peer等信息保存到了comm->p2plist中；
然后执行ncclGroupEnd，如果发现channel没有建立到peer的链接则先建链，
然后根据p2plist执行scheduleSendRecv(ncclSaveKernel)将信息保存到channel->collectives，
然后再启动kernel，kernel会遍历channel->collectives执行send和recv。然后我们看下多机的流程是怎样的。
最后总结下多机通信的整体流程
通信由kernel和proxy线程协调完成，send端kernel负责将数据从input搬运到buf，proxy线程负责将buf中数据通过网络发送给recv端
kernel和proxy间通过队列实现生产者消费者模式
send端通过rdma send发送数据，和recv端通过队列实现生产者消费者模式，队列位于send端，recv端每次下发一个wr到rq之后会执行rdma write通知send端

NVIDIA NCCL 源码学习（九）- 单机内ncclSend和ncclRecv的过程
https://blog.csdn.net/KIDGIN7439/article/details/128326053
单机内的通信都是通过kernel来进行的，所以整个通信的过程可以分为两步，第一步是准备kernel相关的参数，第二步是实际执行kernel的过程。

NVIDIA NCCL 源码学习（八）- 数据通信链路transport的建立
https://blog.csdn.net/kidgin7439/article/details/126953432
最后简单总结下，建链的过程都是以下过程：
接收端 执行recv setup，创建buffer等，将相关信息记录到connectIndo，启动一个监听socket，ip port同样记录到connectInfo，通过bootstrap发送connectInfo到 发送端。
发送端 执行send setup，创建buffer等，将相关信息记录到connectInfo，然后发送给 接收端。这一步rdma场景没有用到connectInfo。
发送端 接受到步骤1中 接收端 的信息，然后建立 发送端 到 接收端 的链接，p2p场景的话只是简单记录对端buffer，rdma场景的话需要初始化qp到INIT状态。
接收端 接受到步骤2中send发送的信息，然后建立 接收端 到 发送端 的链接，p2p场景还是记录对端buffer，rdma场景需要初始化qp到RTS状态，将本端的qp信息发送回对端。
如果rdma场景的话，发送端 还需接收对端的qp状态初始化本端的qp到RTS状态。

NVIDIA NCCL 源码学习（七）- 机器间channel连接
https://blog.csdn.net/KIDGIN7439/article/details/128144057
NVIDIA NCCL 源码学习（六）- channel搜索
https://zhuanlan.zhihu.com/p/653440728
NVIDIA NCCL 源码学习（五）- 路径计算
https://my.oschina.net/oneflow/blog/10089670
NVIDIA NCCL 源码学习（四）- 建图过程
https://zhuanlan.zhihu.com/p/640812018
NVIDIA NCCL 源码学习（三）- 机器内拓扑分析
https://zhuanlan.zhihu.com/p/625606436
NVIDIA NCCL 源码学习（二）- bootstrap网络连接的建立
https://zhuanlan.zhihu.com/p/620499558
NVIDIA NCCL 源码学习（一）- 初始化及ncclUniqueId的产生
https://zhuanlan.zhihu.com/p/614746112


=======================================================================================================================================================

https://blog.csdn.net/weixin_34313182/article/details/92119606
nvidia-nccl 学习笔记
NCCL1 vs NCCL2
nccl1：
nccl1支持单机多卡通信，不支持多机通信。
开源地址：https://github.com/NVIDIA/nccl-tests
nccl2:
nccl2支持多机通信，在nccl1的基础上增加了多机通信策略。多机通信可进行通信协议的选择，支持通过IB、TCP等协议实现多机间数据通信。
NCCL2接口
NCCL 动态扩展
单机多卡多线程动态扩展
设计思路：
采用在线程内各自初始化自己communicator的方法进行初始化（在主线程中创建ncclid，该ncclid对全局线程可见）。当某一个线程调用merge操作失败时，查看是否因为某个线程退出引起的。
如果因为某个线程退出引起merge失败，这时每个线程重新初始化自己的communicator，并进行上一步的merge操作（该次初始化时device已经减少，相当于重新创建communicator）

测试结论：
1. 每个线程初始化自己OK
2. merge操作过程中如果出现某个线程退出，其他线程会处于block状态（不返回）
结论
单机多卡（多线程）动态扩展无法支持。
单机/多机多卡多进程动态扩展
设计思路：
采用在进程内各自初始化自己communicator的方法进行初始化（初始化时，0号进程使用tpc协议广播ncclid到全部进程）。当某一个进程调用merge操作失败时，查看是否是因为有进程退出引起的。 如果因为某个进程退出引起merge失败，这时每个进程重新初始化自己的communicator，并进行上一步的merge操作（该次初始化时device已经减少，相当于重新创建communicator）

测试结论： 1. server进程（TCP server端）创建ncclId，并且将该进程bcast到所有work进程（TCP client端），然后进行通信是可以的（server进程可以不参与通信）
2. merge操作过程中如果出现某个进程退出，其他进程全部处于block状态（不返回），且这时候其他进程的GPU使用率是100%，cpu使用100%。
结论
单机／多机多卡多进程动态扩展无法支持。


=======================================================================================================================================================

https://www.zhihu.com/question/63219175/answer/206697974
如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL？

https://www.zhihu.com/question/63219175
如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL？



https://lgd.gd/2021/03/21/NCCL%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/
NCCL源码阅读笔记

NCCL相关笔记
https://blog.csdn.net/eternal963/article/details/130754512?ops_request_misc=&request_id=&biz_id=102&utm_term=NCCL&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-130754512.nonecase&spm=1018.2226.3001.4449

NCCL、OpenMPI、Gloo对比
https://blog.csdn.net/taoqick/article/details/126449935?ops_request_misc=&request_id=&biz_id=102&utm_term=NCCL&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-126449935.nonecase&spm=1018.2226.3001.4449



PS C:\yk_repo\NCCL\v1.3.4-1> git tag
inc_nsteps_v2.16
v1.2.1-1+cuda7.5
v1.2.1-1+cuda7.5-1
v1.2.1-1+cuda8.0
v1.2.1-1+cuda8.0-1
v1.2.1-2+cuda7.5
v1.2.2-1+cuda7.5
v1.2.2-1+cuda8.0
v1.2.3-1+cuda7.5
v1.2.3-1+cuda8.0
v1.3.0-1
v1.3.4-1
v2.10.3-1
v2.11.4-1
v2.12.10-1
v2.12.12-1
v2.12.7-1
v2.13.4-1
v2.14.3-1
v2.15.1-1
v2.15.5-1
v2.16.2-1
v2.16.5-1
v2.17.1-1
v2.18.1-1
v2.18.3-1
v2.18.5-1
v2.3.5-5
v2.3.7-1
v2.4.2-1
v2.4.6-1
v2.4.7-1
v2.4.8-1
v2.5.6-1
v2.5.6-2
v2.5.7-1
v2.6.4-1
v2.7.3-1
v2.7.5-1
v2.7.6-1
v2.7.8-1
v2.8.3-1
v2.8.4-1
v2.9.6-1
v2.9.8-1
v2.9.9-1
PS C:\yk_repo


ncclResult_t ncclSaveKer  最后出现的版本是 v2.8.4-1

bootstrapNetGetSocketAddr
最先出现的版本是 v2.4.8-1(没有出现在v2.4.7-1)
最后出现的版本是 v2.7.8-1(没有出现在v2.8.3-1)